---
layout: post
title:      "Module 1 Project - Dealing with Null Values"
date:       2019-06-24 23:51:11 +0000
permalink:  module_1_project_-_dealing_with_null_values
---


	So for my first project the mission was to use the King County Housing Data set on Kaggle to build a multivariate regression to predict housing price. The data set contained several independent variables such as year built, grade, condition, and view to name a few. Of course, the first step in dealing with a data set is to start cleaning it. In this post I’m going to go over how I started cleaning the data and dealt with the null values.
	So first I used my trusty ```.isnull().any()``` search. This turned up three columns – waterfront, view, and yr_renovated. The easy answer would be just to drop all the offending columns. But I wanted to do some research first. I just bought my first house last year, and things like view and year renovated would have been things that I wanted to pay attention to myself. So dropping them without research was out of the question. 
	First I took a look at the ‘waterfront’ column. It assigned either a true or false value whether the property was waterfront or not. What really interested me is whether being a waterfront property would affect the price though. I plotted a bar graph to look at the mean value of false (not waterfront) and the mean value of true (waterfront.) I found that it did raise the mean value of the price quite a bit to be a waterfront property. So I should keep the variable right? 
	Not necessarily. The second part of my research on this variable was what proportion of the data was actually a null value. I found that 99% of the data points were not waterfront. If I wanted to keep the data, my best option was to make an uneducated guess as to whether the null values were waterfront or not. I decided to drop the column altogether. Being a waterfront property affected the price for sure, but there just wasn’t enough frequency of them for me to justify keeping null values in my data set. 
	Next I moved on to the year renovated column. Again I plotted a bar graph to see if there was any clear indication that year renovated affected price. The bar graph was a little “all over the map.” (No pun intended.) There was nothing to suggest that newer renovations increased the price of the house, but I still had to get rid of my null data points. Something that I did notice by running a ```.value_counts()``` was that there was a very high proportion of 0 values – houses that had never been renovated. This influenced my decision on how to proceed. I couldn’t justify dropping the column – it was still very valuable to me. But I also couldn’t leave the values null. I decided to fill the data. I had to make a decision between mean and median, and that’s where the 0 values comes in. If I took the mean, the zeroes would drag the mean artificially low. Whereas the median would be unaffected by the extra zeroes. Indeed, the median turned out to be 0, so I filled in my null values. 
	Finally, I moved on to the view column. I noticed that it was a categorical variable – each home had a value assigned as 0, 1, 2, 3, or 4. A quick bar graph of price vs. view showed that the home’s price increased with each step up in view. I knew that I couldn’t just drop this variable, it was clearly affecting the housing prices. I decided to fill in this variable as well using my best guess. Again I was confronted with the question of mean or median. I decided on median because of the column being a categorical variable. If I took the mean, I would probably end up with a float that wouldn’t make sense in my data set. So I decided to take the median and fill that in for my null values. Again the median ended up being 0, but I was satisfied that filling in 0 would be the best guess. 
	After dealing with my null values and finishing up the cleaning process, it was time to move on to the linear regression. As I started working on min max scaling, I kept running into an error in the sqft_basement column (square footage of basements.) A little research later, and I found that there were 454 instances of the basement’s square footage being recorded as a question mark. I had found a hidden null column. First, I had to convert the question marks into a NaN value that I could use. I used ```df['sqft_basement'] = df['sqft_basement'].replace(to_replace = "?",value = np.nan)```, which converted all question marks into NaNs. Next, I had to decide if I would fill the null values or drop them. Because sqft_basement seemed to have an effect on price, and there were too many values to justify dropping, I decided to fill in the NaNs. There was a large proportion of 0 values, leading me to believe that most homes in King County don’t have basements. I felt it was an educated guess to fill my missing values with 0. 
	Dealing with the null values actually turned out to be one of the more fun and educational aspects of this project. It involved searching in different ways, i.e. for NaN and also question marks. But it also took extra research and knowledge of Numpy and Pandas in order to fill in the missing values. I particularly enjoyed the decision making aspect of data filling. This was a good opportunity to sharpen my skills in data cleaning. 
